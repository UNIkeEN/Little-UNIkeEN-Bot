import itertools
from torch.utils import data as dataimport
import re
from time import sleep
import jieba
import torch
import logging
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.utils as utils
from typing import Union, Any
from utils.basicEvent import *
from utils.basicConfigs import *
from utils.functionConfigs import check_config, check_config_mode
from utils.standardPlugin import StandardPlugin
jieba.setLogLevel(logging.INFO) #å…³é—­jiebaè¾“å‡ºä¿¡æ¯

class NLP_Config:
    '''
    æ•°æ®é›†ç›¸å…³
    '''
    corpus_data_path = 'resources/corpus/processed.pth' 
    shuffle = True
    load_checkpoint = 'resources/corpus/checkpoint_0827_0938.pth'
    #load_checkpoint = 'NLP/model_save/checkpoint_0821_1253.pth'
    #load_checkpoint = None
    max_input_length = 50 #è¾“å…¥çš„æœ€å¤§å¥å­é•¿åº¦
    max_generate_length = 20 #ç”Ÿæˆçš„æœ€å¤§å¥å­é•¿åº¦
    '''
    è®­ç»ƒè¶…å‚æ•°
    '''
    dim_embedding = 256 # è¯åµŒå…¥ç»´æ•°
    num_layer = 2 # Encoder-Decoderä¸­RNNçš„å±‚æ•°
    hidden_size = 256 # éšè—å±‚å¤§å°
    batch_size = 2048
    encoder_lr = 1e-3 # encoderå­¦ä¹ ç‡
    decoder_lr = 5e-3 # decoderå­¦ä¹ ç‡
    grad_clip = 50.0 # æ¢¯åº¦è£å‰ª
    teacher_forcing_ratio = 1.0 # teacher_forcingçš„æ¯”ä¾‹
    '''
    è®­ç»ƒå‘¨æœŸç›¸å…³
    '''
    num_epoch = 50
    save_epoch = 50
    '''
    è®¾å¤‡ç›¸å…³
    '''
    is_cuda = torch.cuda.is_available()
    device = "cuda:0" if is_cuda else "cpu" 

# æ¨¡å‹å®šä¹‰
class Encoder_RNN(nn.Module):
    def __init__(self, conf, len_vocab):
        super(Encoder_RNN, self).__init__()
        # RNNå±‚æ•°ä¸éšè—å±‚å¤§å°
        self.num_layer = conf.num_layer
        self.hidden_size = conf.hidden_size
        # embçš„è¾“å…¥ä¸ºå­—å…¸é•¿åº¦ï¼ˆè¯çš„ä¸ªæ•°ï¼‰
        self.embedding = nn.Embedding(len_vocab, conf.dim_embedding)
        # åŒå±‚GRU
        self.gru = nn.GRU(
            input_size = conf.dim_embedding,
            hidden_size = self.hidden_size,
            num_layers = self.num_layer,
            bidirectional = True # ä½¿ç”¨åŒå‘GRU
        )

    def forward(self, input_seq, input_lengths, hidden = None): # åˆå§‹hiddenä¸ºç©º
        '''
        input_seq:è¾“å…¥åºåˆ—
            size: [max_seq_len, batch_size]
        input_lengths:è¾“å…¥åºåˆ—é•¿åº¦,æŸä¸€batchå†…æ¯ä¸ªå¥å­çš„é•¿åº¦åˆ—è¡¨
            size: [batch_size]
        '''
        # è¯åµŒå…¥
        embedded = self.embedding(input_seq)
        '''
        embedded:è¯åµŒå…¥å¤„ç†å
            size: [max_seq_len, batch_size, dim_embedding]
        '''
        # æŒ‰ç…§åºåˆ—é•¿åº¦åˆ—è¡¨å¯¹çŸ©é˜µè¿›è¡Œå‹ç¼©,åŠ å¿«RNNçš„è®¡ç®—æ•ˆç‡
        packed_data = utils.rnn.pack_padded_sequence(embedded, input_lengths)
        output, hidden = self.gru(packed_data, hidden)
        # è§£å‹ç¼©ä¸ºå®šé•¿åºåˆ—çŸ©é˜µ
        output, _ = utils.rnn.pad_packed_sequence(output)
        '''
        hidden:éšè—å±‚,åˆå§‹å€¼ä¸ºNone
            size: [num_layers*num_directions, batch_size, hidden_size]
            åŒå‘GRU,å…¶ç¬¬ä¸€ä¸ªç»´åº¦ä¸ºä¸åŒæ–¹å‘çš„å åŠ 
        output:è¾“å‡ºå±‚
            size: [max_seq_len, batch_size, num_directions*hidden_size]
        '''
        # å¯¹å‰åä¸¤ä¸ªæ–¹å‘æ±‚å’Œè¾“å‡º
        output = output[:,:,:self.hidden_size]+output[:,:,self.hidden_size:]
        '''
        output.size->[max_seq_len, batch_size, hidden_size]
        '''
        return output, hidden
class Decoder_RNN(nn.Module):
    def __init__(self, conf, len_vocab):
        super(Decoder_RNN, self).__init__()
        # RNNå±‚æ•°å’Œéšè—å±‚å¤§å°
        self.num_layer = conf.num_layer
        self.hidden_size = conf.hidden_size
        # embçš„è¾“å…¥ä¸ºå­—å…¸é•¿åº¦ï¼ˆè¯çš„ä¸ªæ•°ï¼‰
        self.embedding = nn.Embedding(len_vocab, conf.dim_embedding)
        # GRU
        self.gru = nn.GRU(
            input_size = conf.dim_embedding,
            hidden_size = self.hidden_size,
            num_layers = self.num_layer,
        )
        # concatå±‚ä¸è¾“å‡ºå±‚
        self.concat = nn.Linear(self.hidden_size*2, self.hidden_size)
        self.out = nn.Linear(self.hidden_size, len_vocab)

    def forward(self, input, hidden, encoder_hiddens):
        '''
        input:è¾“å…¥
            decoderé€å­—ç”Ÿæˆ,åä¸€ä¸ªæ—¶é—´æ­¥æ¥å—å‰ä¸€ä¸ªæ—¶é—´æ­¥ç”Ÿæˆçš„å­—ã€‚ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥æ¥å—å¥å­å¼€å§‹çš„ç¬¦å·
            å³æ¥å—input=å¼€å§‹ç¬¦ç´¢å¼•
            shape:[1, batch_size]
        '''
        # è¯åµŒå…¥
        embedded = self.embedding(input)
        '''
        embedded:è¯åµŒå…¥å¤„ç†å
            size: [1, batch_size, dim_embedding]
        '''
        # RNN
        output, hidden = self.gru(embedded, hidden)
        '''
        hidden:éšè—å±‚,æœ€æ—©ä¼ å…¥çš„æ˜¯encoderæœ€åæ—¶åˆ»çš„è¾“å‡ºå±‚,encoder_hiddençš„æ­£å‘éƒ¨åˆ†
            size: [num_layers, batch_size, hidden_size]
            åŒå‘GRU,å…¶ç¬¬ä¸€ä¸ªç»´åº¦ä¸ºä¸åŒæ–¹å‘çš„å åŠ 
        output:è¾“å‡ºå±‚
            size: [max_seq_len, batch_size, hidden_size]
        '''
        # dotæ–¹å¼è®¡ç®—Attention
        '''
        encoder_outputs:
            encoderæ‰€æœ‰æ—¶é—´æ­¥çš„hiddenè¾“å‡º
            shape: [max_seq_len, batch_size, hidden_size]
        '''
        dot_score = torch.sum(output* encoder_hiddens, dim=2).t()
        dot_score = F.softmax(dot_score, dim=1).unsqueeze(1)
        '''
        dot_score: attentionçš„å¾—åˆ†
            shape: [max_seq_len, batch_size]
            è½¬ç½®->[batch_size, max_seq_len]
            å¢åŠ ç»´åº¦->[batch_size, 1, max_seq_len]
        '''
        # æ‰¹é‡ç›¸ä¹˜ï¼Œå½¢æˆcontext
        context = dot_score.bmm(encoder_hiddens.transpose(0,1))
        context = context.squeeze(1)
        '''
        context:
            shape: [batch_size, hidden_size]
        '''
        output = output.squeeze(0) # [batch_size, hidden_size]
        # æ‹¼æ¥outputå’Œcontextï¼Œé€šè¿‡çº¿æ€§å±‚å˜ä¸ºå•å±‚
        concat_input = torch.cat((output, context), 1)
        concat_output = torch.tanh(self.concat(concat_input)) 
        # è¾“å‡ºä¸softmaxå½’ä¸€åŒ–
        final_output = self.out(concat_output)
        final_output = F.softmax(final_output, dim=1)

        return final_output, hidden

class GreedySearchDecoder(nn.Module):
    def __init__(self, encoder, decoder):
        super(GreedySearchDecoder, self).__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, sos, eos, input_seq, input_length, max_length, device):

        # Encoderçš„Forwardè®¡ç®— 
        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)
        # æŠŠEncoderæœ€åæ—¶åˆ»çš„éšçŠ¶æ€ä½œä¸ºDecoderçš„åˆå§‹å€¼
        decoder_hidden = encoder_hidden[:self.decoder.num_layer]
        # å› ä¸ºæˆ‘ä»¬çš„å‡½æ•°éƒ½æ˜¯è¦æ±‚(time,batch)ï¼Œå› æ­¤å³ä½¿åªæœ‰ä¸€ä¸ªæ•°æ®ï¼Œä¹Ÿè¦åšå‡ºäºŒç»´çš„ã€‚
        # Decoderçš„åˆå§‹è¾“å…¥æ˜¯SOS
        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * sos
        # ç”¨äºä¿å­˜è§£ç ç»“æœçš„tensor
        all_tokens = torch.zeros([0], device=device, dtype=torch.long)
        all_scores = torch.zeros([0], device=device)
        # å¾ªç¯ï¼Œè¿™é‡Œåªä½¿ç”¨é•¿åº¦é™åˆ¶ï¼Œåé¢å¤„ç†çš„æ—¶å€™æŠŠEOSå»æ‰äº†ã€‚
        for _ in range(max_length):
            # Decoder forwardä¸€æ­¥
            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, 
								encoder_outputs)
            # decoder_outputsæ˜¯(batch=1, vob_size)
            # ä½¿ç”¨maxè¿”å›æ¦‚ç‡æœ€å¤§çš„è¯å’Œå¾—åˆ†
            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)
            # æŠŠè§£ç ç»“æœä¿å­˜åˆ°all_tokenså’Œall_scoresé‡Œ
            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)
            all_scores = torch.cat((all_scores, decoder_scores), dim=0)
            # decoder_inputæ˜¯å½“å‰æ—¶åˆ»è¾“å‡ºçš„è¯çš„IDï¼Œè¿™æ˜¯ä¸ªä¸€ç»´çš„å‘é‡ï¼Œå› ä¸ºmaxä¼šå‡å°‘ä¸€ç»´ã€‚
            # ä½†æ˜¯decoderè¦æ±‚æœ‰ä¸€ä¸ªbatchç»´åº¦ï¼Œå› æ­¤ç”¨unsqueezeå¢åŠ batchç»´åº¦ã€‚
            if decoder_input.item() == eos:
                break
            decoder_input = torch.unsqueeze(decoder_input, 0)
            
        # è¿”å›æ‰€æœ‰çš„è¯å’Œå¾—åˆ†ã€‚
        return all_tokens, all_scores

# data_loaderç›¸å…³
def zipAndPadding(lst, pad):
    ret = itertools.zip_longest(*lst, fillvalue=pad)
    return list(ret)
def maskMatrix(lst, pad):
    mask = []
    for i, seq in enumerate(lst):
        mask.append([])
        for token in seq:
            if token == pad:
                mask[i].append(0)
            else:
                mask[i].append(1)
    return mask
def create_collate_fn(padding, eos):
    '''
    è¯´æ˜dataloaderå¦‚ä½•åŒ…è£…ä¸€ä¸ªbatch,ä¼ å…¥çš„å‚æ•°ä¸º</PAD>çš„ç´¢å¼•padding,</EOS>å­—ç¬¦ç´¢å¼•eos
    collate_fnä¼ å…¥çš„å‚æ•°æ˜¯ç”±ä¸€ä¸ªbatchçš„__getitem__æ–¹æ³•çš„è¿”å›å€¼ç»„æˆçš„corpus_item

    corpus_item: 
        lsit, å½¢å¦‚[(inputVar1, targetVar1, index1),(inputVar2, targetVar2, index2),...]
        inputVar1: [word_ix, word_ix, word_ix,...]
        targetVar1: [word_ix, word_ix, word_ix,...]
    inputs: 
        å–å‡ºæ‰€æœ‰inputVarç»„æˆçš„list,å½¢å¦‚[inputVar1,inputVar2,inputVar3,...], 
        paddingå(è¿™é‡Œæœ‰éšå¼è½¬ç½®)è½¬ä¸ºtensoråå½¢çŠ¶ä¸º:[max_seq_len, batch_size]
    targets:
        å–å‡ºæ‰€æœ‰targetVarç»„æˆçš„list,å½¢å¦‚[targetVar1,targetVar2,targetVar3,...]
        paddingå(è¿™é‡Œæœ‰éšå¼è½¬ç½®)è½¬ä¸ºtensoråå½¢çŠ¶ä¸º:[max_seq_len, batch_size]
    input_lengths: 
        åœ¨paddingå‰è¦è®°å½•åŸæ¥çš„inputVarçš„é•¿åº¦, ç”¨äºpad_packed_sequence
        å½¢å¦‚: [length_inputVar1, length_inputVar2, length_inputVar3, ...]
    max_targets_length:
        è¯¥æ‰¹æ¬¡çš„æ‰€æœ‰targetçš„æœ€å¤§é•¿åº¦
    mask:
        å½¢çŠ¶: [max_seq_len, batch_size]
    indexes:
        è®°å½•ä¸€ä¸ªbatchä¸­æ¯ä¸ª å¥å­å¯¹ åœ¨corpusæ•°æ®é›†ä¸­çš„ä½ç½®
        å½¢å¦‚: [index1, index2, ...]

    '''
    def collate_fn(corpus_item):
        #æŒ‰ç…§inputVarçš„é•¿åº¦è¿›è¡Œæ’åº,æ˜¯è°ƒç”¨pad_packed_sequenceæ–¹æ³•çš„è¦æ±‚
        corpus_item.sort(key=lambda p: len(p[0]), reverse=True) 
        inputs, targets, indexes = zip(*corpus_item)
        input_lengths = torch.tensor([len(inputVar) for inputVar in inputs])
        inputs = zipAndPadding(inputs, padding)
        inputs = torch.LongTensor(inputs) #æ³¨æ„è¿™é‡Œè¦LongTensor
        max_target_length = max([len(targetVar) for targetVar in targets])
        targets = zipAndPadding(targets, padding)
        mask = maskMatrix(targets, padding)
        mask = torch.tensor(mask, dtype=bool)
        targets = torch.LongTensor(targets)
        
        return inputs, targets, mask, input_lengths, max_target_length, indexes

    return collate_fn
class CorpusDataset(dataimport.Dataset):

    def __init__(self, conf):
        self.conf = conf
        self._data = torch.load(conf.corpus_data_path)
        self.word2ix = self._data['word2ix']
        self.corpus = self._data['corpus']
        self.padding = self.word2ix.get(self._data.get('pad'))
        self.eos = self.word2ix.get(self._data.get('eos'))
        self.sos = self.word2ix.get(self._data.get('sos'))
        
    def __getitem__(self, index):
        inputVar = self.corpus[index][0]
        targetVar = self.corpus[index][1]
        return inputVar, targetVar, index

    def __len__(self):
        return len(self.corpus)

def get_dataloader(conf):
    dataset = CorpusDataset(conf)
    dataloader = dataimport.DataLoader(dataset,
                                 batch_size=conf.batch_size,
                                 shuffle=conf.shuffle, #æ˜¯å¦æ‰“ä¹±æ•°æ®
                                 drop_last=True, #ä¸¢æ‰æœ€åä¸€ä¸ªä¸è¶³ä¸€ä¸ªbatchçš„æ•°æ®
                                 collate_fn=create_collate_fn(dataset.padding, dataset.eos))
    return dataloader

# åŠ è½½æ¨¡å‹ç±»
class EvalModel():
    def __init__(self):
        self.load_net()

    def load_net(self):
        self.conf = NLP_Config()
        # åŠ è½½æ•°æ®
        dataloader = get_dataloader(self.conf)
        self._data = dataloader.dataset._data
        self.word2ix, self.ix2word = self._data['word2ix'], self._data['ix2word']
        self.sos = self.word2ix.get(self._data.get('sos'))
        self.eos = self.word2ix.get(self._data.get('eos'))
        self.unk = self.word2ix.get(self._data.get('unk'))
        self.len_vocab = len(self.word2ix)

        self.encoder = Encoder_RNN(self.conf, self.len_vocab)
        self.decoder = Decoder_RNN(self.conf, self.len_vocab)

        checkpoint = torch.load(self.conf.load_checkpoint, map_location='cpu')
        self.encoder.load_state_dict(checkpoint['encoder'])
        self.decoder.load_state_dict(checkpoint['decoder'])

        with torch.no_grad():
            #åˆ‡æ¢æ¨¡å¼
            self.encoder = self.encoder.to(self.conf.device)
            self.decoder = self.decoder.to(self.conf.device)
            self.encoder.eval()
            self.decoder.eval()
            #å®šä¹‰seracher
            self.searcher = GreedySearchDecoder(self.encoder, self.decoder)
    
    def eval(self,input_sentence):
        cop = re.compile("[^\u4e00-\u9fa5^a-z^A-Z^0-9]") #åˆ†è¯å¤„ç†æ­£åˆ™
        input_seq = jieba.lcut(cop.sub("",input_sentence)) #åˆ†è¯åºåˆ—
        input_seq = input_seq[:self.conf.max_input_length] + ['</EOS>']
        input_seq = [self.word2ix.get(word, self.unk) for word in input_seq]
        tokens = self.generate(input_seq, self.searcher, self.sos, self.eos, self.conf)
        output_words = ''.join([self.ix2word[token.item()] for token in tokens])
        return output_words


    def generate(self, input_seq, searcher, sos, eos, opt):
        #input_seq: å·²åˆ†è¯ä¸”è½¬ä¸ºç´¢å¼•çš„åºåˆ—
        #input_batch: shape: [1, seq_len] ==> [seq_len,1] (å³batch_size=1)
        input_batch = [input_seq]
        input_lengths = torch.tensor([len(seq) for seq in input_batch])
        input_batch = torch.LongTensor([input_seq]).transpose(0,1)
        input_batch = input_batch.to(opt.device)
        input_lengths = input_lengths.to(opt.device)
        tokens, scores = searcher(sos, eos, input_batch, input_lengths, opt.max_generate_length, opt.device)
        return tokens

NLP_model = EvalModel()

class ChatWithNLP(StandardPlugin): # NLPå¯¹è¯æ’ä»¶
    def judgeTrigger(self, msg:str, data:Any) -> bool:
        return startswith_in(msg, ['å°ğŸ¦„ï¼Œ','å°é©¬ï¼Œ','å°ğŸ¦„,','å°é©¬,']) and check_config_mode(data['group_id'],'Auto_Answer')=='nlp'
    def executeEvent(self, msg:str, data:Any) -> Union[None, str]:
        if data['message_type']=='group' and not check_config(data['group_id'],'Auto_Answer'):
            send(data['group_id'],TXT_PERMISSION_DENIED)
        target = data['group_id'] if data['message_type']=='group' else data['user_id']
        msg_inp = msg[3:]
        ret = NLP_model.eval(msg_inp)
        ret = ret.replace('</EOS>','',1).replace('</UNK>',' ').strip()
        if ret=="":
            ret = "æˆ‘å¥½åƒä¸æ˜ç™½æqwq"
        text = f'[CQ:reply,id='+str(data['message_id'])+']'+ret
        send(target, text, data['message_type'])
        sleep(0.3)
        # if ret != "æˆ‘å¥½åƒä¸æ˜ç™½æqwq":
        #     voice = send_genshin_voice(ret+'ã€‚')
        #     send(target, f'[CQ:record,file=files://{ROOT_PATH}/{voice}]', data['message_type'])
        return "OK"
    def getPluginInfo(self, )->Any:
        return {
            'name': 'ChatWithNLP',
            'description': 'NLPå¯¹è¯',
            'commandDescription': 'å°é©¬ï¼Œ',
            'usePlace': ['group', 'private', ],
            'showInHelp': True,
            'pluginConfigTableNames': [],
            'version': '1.0.0',
            'author': 'Unicorn',
        }